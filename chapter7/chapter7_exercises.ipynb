{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Inference for numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1 - Identify the critical $t$.\n",
    "\n",
    "* (a) $df = 6 - 1 = 5,\\ t_{5} = 2.015$\n",
    "* (b) $df = 21 - 1 = 20,\\ t_{20} = 2.528$\n",
    "* (c) $df = 29 - 1 = 28,\\ t_{28} = 2.048$\n",
    "* (d) $df = 12 - 1 = 11,\\ t_{11} = 3.106$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2 - $t$-distribution.\n",
    "\n",
    "Given that the more degree of freedoms, the more the distribution becomes normal, we can say that dotted is the distribution with the less degrees of freedom, i.e. with one degree of freedom, dashed is the one with 5 degrees of freedom and the solid one is the normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercixe 7.3 - Find the p-value, Part I.\n",
    "\n",
    "* (a) Null hypothesis not rejected.\n",
    "* (b) Null hypothesis rejected.\n",
    "* (c) Null hypothesis not rejected.\n",
    "* (d) Null hypothesis rejected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercixe 7.4 - Find the p-value, Part II.\n",
    "\n",
    "* (a) Null hypothesis not rejected.\n",
    "* (b) Null hypothesis not rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6234852065657115, False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "a = 0.01\n",
    "n = 18\n",
    "T = 0.5\n",
    "\n",
    "df = n - 1\n",
    "\n",
    "(1 - t.cdf(abs(T), df)) * 2, (1 - t.cdf(abs(T), df)) * 2 < a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 7.5 - Working backwards, Part I.\n",
    "\n",
    "Sample mean is simply $\\bar{x} = 20$ and standard deviation is $\\sigma = 2.99$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 7.6 - Working backwards, Part II.\n",
    "\n",
    "Sample mean is $\\bar{x} = 61$, standard deviation is $\\sigma = 17.53$ and margin of error is $MOE = t\\_statistic * SE = 1.71 * SE = 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.7 - Sleep habits of New Yorkers.\n",
    "\n",
    "* (a) Null hypothesis says that New Yorkers sleep 8 hours at night ($\\mu_{NY} = 8$). Alternate hypothesis says that New Yorkers sleep a different amount of time ($\\mu_{NY} \\neq 8$).\n",
    "* (b) Normality seems to be met, $SE = s / \\sqrt{n} = 0.77 / 5 = 0.154$, $df = 25 - 1 = 24$, and the T statistic is $T = -1.75$ which leads us to a p-value of $0.092$.\n",
    "* (c) The p-value tells how much is the probability of having given this result by mere chance, and has to be less than our significance level in order to reject the null hypothesis.\n",
    "* (d) The difference is not significant to reject the null hypothesis.\n",
    "* (e) Given the result of the p-value, I would not expect 8 to be in the interval if we choose a significance level of 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.8 - Heights of adults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (a) 171.1 cm and 170.3 cm.\n",
    "* (b) 9.4 and (163.8, 177.8).\n",
    "* (c) A person of 180 cm has a T score of 0.947 therefore he/she is not unusual, while a person of 155 cm is more unusual having a T score of -1.71.\n",
    "* (d) No, since we would have other sample statistics differing from the above.\n",
    "* (e) We would use the standard error calculated as $SE = \\bar{\\sigma} / \\sqrt{n} = 3.066$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.9 - Find the mean.\n",
    "\n",
    "The requested sample mean would be $56.26$ or $63.74$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.10 - $t⋆$ vs. $z⋆$.\n",
    "\n",
    "The confidence interval will be itself larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.11 - Play the piano.\n",
    "\n",
    "* (a) The null hypothesis tells that Georgianna is wrong since the average years of piano lessons are not different from the global average of 5. The alternate hypothesis would give credit to Georgianna claim.\n",
    "* (b) Derived confidence interval is $[3.57,\\ 5.62]$, and $pval = 0.43$ so we cannot reject the null hypothesis.\n",
    "* (c) They agree since there is a huge overlap between the null mean and the alternate mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.12 - Auto exhaust and lead exposure.\n",
    "\n",
    "* (a) Null hypothesis is that the obtained result is only gotten by chance ($H_0: \\mu = 35$) while alternate hypothesis is that there is a significant difference in the actual levels ($H_0: \\mu \\neq 35$).\n",
    "* (b) Since $n > 30$, we can assume normality.\n",
    "* (c) We got a p value of almost zero, so we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.13 - Car insurance savings.\n",
    "\n",
    "He would need to interview around 384/385 people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.14 - SAT scores.\n",
    "\n",
    "* (a) She would need a sample of 273.\n",
    "* (b) He would definitely need a bigger sample.\n",
    "* (c) The minimum sample size would be 664."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.15 - Air quality.\n",
    "\n",
    "We can use a paired test if for each capital we have a sample for both 2013 and 2014. It would make sense to use paired data to perform our inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.16 - True / False: paired.\n",
    "\n",
    "* (a) True.\n",
    "* (b) True.\n",
    "* (c) True.\n",
    "* (d) False, since each observation of the first dataset is subtracted from the corresponding observation in the second dataset or viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.17 - Paired or not? Part I.\n",
    "\n",
    "* (a) Paired, since for each student we have a pre-semester grade and a post-semester grade.\n",
    "* (b) Unpaired, since this is about the difference of two different populations.\n",
    "* (c) Paired, since we are measuring the same measure over time for the same sample of patients.\n",
    "* (d) Paired, since we would measure the weight of the same individuals before and after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.18 - Paired or not? Part II.\n",
    "\n",
    "* (a) Unpaired, since they are two different populations.\n",
    "* (b) Paired, since the same items price are measured in different places.\n",
    "* (c) Unpaired, since the populations are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.19 - Global warming, Part I.\n",
    "\n",
    "* (a) They are paired since for each observation we are matching the temperatured as measured in 1948 and 2018.\n",
    "* (b) Null hypothesis is that the number of days with temperature exceeding 90 degrees did not change from 1948 to 2018 ($H_0: \\mu_{1948} = \\mu_{2018}$). Alternate hypothesis is that the number of days with temperature exceeding 90 degrees did change ($H_0: \\mu_{1948} \\ne \\mu_{2018}$). \n",
    "* (c) Normality can be assumed as the sample is big and there are no extreme outliers.\n",
    "* (d) First we calculate $SE = \\frac{sd}{\\sqrt{n}} = 0.21$. We can calculate the t-statistic as $t = \\frac{2.9}{17.2} = 2.36$ with $p\\_value = 0.01$.\n",
    "* (e) The p-value makes us reject the null hypothesis in favor of the alternate hypothesis.\n",
    "* (f) We might have made a type I error where we rejected a true null hypothesis.\n",
    "* (g) I wouldn't expect the value $0$ included in my confidence interval, and indeed it is not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.20 - High School and Beyond, Part I.\n",
    "\n",
    "* (a) There seems to be a slight difference in the average reading and writing scores.\n",
    "* (b) From the left plot, I can see they are somehow generally related.\n",
    "* (c) $H_0: \\mu_{read} - \\mu_{write} = 0$ and $H_A: \\mu_{read} - \\mu_{write} \\ne 0$.\n",
    "* (d) Sample is random, there are no extreme outliers for the sample size so normality can be assumed.\n",
    "* (e) Let's calculate the _Standard Error_ $SE = 0.628$. Let's calculate the _t-statistic_ as $t = -0.867$ which leads us to $p\\_value = 0.387$.\n",
    "* (f) We might have made a _Type II Error_, that is failing to reject a null hypothesis which is actually true. So we might actually have a significant difference between reading and writing scores, but the data failed to highlight it.\n",
    "* (g) I would definitely expect the value $0$ to be included."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.21 - Global warming, Part II.\n",
    "\n",
    "* (a) The 90% confidence interval is $[2.63,\\ 3.17]$.\n",
    "* (b) We are 90% confident that the true average difference is in the above interval.\n",
    "* (c) Yes, since the interval is delimited by two positive values showing the actual presence of a difference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.22 - High school and beyond, Part II.\n",
    "\n",
    "* (a) The 95% confidence interval is $[-1.583,\\ 0.493]$.\n",
    "* (b) We are 95% confidence that the true population difference between read and write scores is within the above interval.\n",
    "* (c) No, it does not, since it includes the $0$ value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.23 - Friday the 13<sup>th</sup>, Part I.\n",
    "\n",
    "* (a) Data is paired: they refer to the number of cars passing by a specific intersection on both days.\n",
    "* (b) $H_0: n_{6^{th}} - n_{13^{th}} = 0$ while $H_A: n_{6^{th}} - n_{13^{th}} \\ne 0$. Basically in the null hypothesis we are assuming that there is no actual difference between the two numbers while in the alternate hypothesis we are saying there is difference.\n",
    "* (c) We are in a particular situation where assuming normality is a very strong assumption and given the outliers, these have to be reported.\n",
    "* (d) We have $T = 4.93$ and $p\\_value = 0.001$.\n",
    "* (e) We can reject the null hypothesis. We have strong evidence that the average number of cars at the intersection is greater on 6<sup>th</sup>.\n",
    "* (f) If the average number of cars passing were the same for both days the probability to see such a test statistic would be less than $0.01$.\n",
    "* (g) We might have made a _Type I Error_ that is to reject a null hypothesis that should not be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "def se_diff(s1, s2, n1, n2):\n",
    "    return ((s1 ** 2) / n1 + (s2 ** 2) / n2) ** 0.5\n",
    "\n",
    "def df_diff(n1, n2):\n",
    "    return min(n1 - 1, n2 - 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.24 - Diamonds, Part I.\n",
    "\n",
    "Before proceeding with our testing, we need to assess normality. A sample size of 23 will require that no big outliers are present and if so, they ought to be reported when stating the results. From the box plots, we can see that there are some outliers, and this has to be reported.\n",
    "\n",
    "We are comparing two means, so we will use the difference of two means inference framework.\n",
    "Let denote with $\\mu_{diff} = \\mu_{1.00} - \\mu_{0.99} = 12.3$ the difference between the standardized prices for 1-carat diamonds and 0.99-carat diamonds. Let us calculate the _Standard Error_ as $SE = 4.36$ and degrees of freedom $df = 23 - 1 = 22$.\n",
    "\n",
    "Let's now move on calculating $T = 2.82$ which leads to $p\\_value = 0.01$ which allows us to reject the null hypothesis and affirm that there is a significant difference in the standardized prices, which by logic there should not be."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.25 - Friday the 13th, Part II.\n",
    "\n",
    "* (a) Normality is a very strong assumption here given the small number of samples. We have to note that and state this when we conclude our analysis. With that in mind, let's state the two hypothesis: $H_0: \\mu_{13^{th}} - \\mu_{6^{th}} = 0,\\ H_A: \\mu_{13^{th}} - \\mu_{6^{th}} \\ne 0$. Let's find the _Standard Error_ as $SE = 1.23$ with a _T Statistic_ of $T = 2.71$. Let's calculate the _p-value_ as $p\\_value = 0.04$ which makes us reject the null hypothesis. We might have made a _Type I Error_ here.\n",
    "\n",
    "* (b) Let's find the critical value first $t_{95} = 2.57$ and then let's define our _95% Confidence Interval_ as $[0.17,\\ 6.49]$ which means that we are 95% confident that on average there are between 0.17 and 6.49 more admissions to ER during the 13<sup>th</sup> compare to the 6<sup>th</sup>.\n",
    "\n",
    "* (c) The dataset is too small to deduct any conclusion anyway. The 95% confidence interval indeed show us 0 as a value that might be plausible. I would refrain from certain conclusions based on the data at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.26 - Diamonds, Part II.\n",
    "\n",
    "Let's find the critical value first $T = 2.07$ and the _95% Confidence Interval_ is $[3.25,\\ 21.35]$. So we are 95% confident that on average 1.00-carat diamonds costs (per carat) more than 0.99-carat diamons by an amount between the given confidence interval."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.27 - Chicken diet and weight, Part I.\n",
    "\n",
    "* (a) There is clear difference between the two groups, wiht linseed-fed chickens being heavier. \n",
    "* (b) Let's calculate the _Standard Error_ as $SE = 19.41$, the _degrees of freedom_ as $df = 9$, $T = 3.02$ and finally we can compute $p\\_value = 0.014$. So data suggest that there is a significant weight difference.\n",
    "* (c) We might have committed a _Type I Error_ that is rejecting a true Null Hypothesis.\n",
    "* (d) With $\\alpha = 0.01$ we would not reject the Null Hypothesis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.28 - Fuel efficiency of manual and automatic cars, Part I.\n",
    "\n",
    "Let's calculate $SE = 1.13$ and $df = 25$. Let's move on calculating the _T Statistic_ as $T = 3.30$ which leads to a p-value of $p\\_value = 0.003$. Now, assuming the hypothesis are $H_0: \\mu_{manual} - \\mu_{automatic} = 0$ and $H_A: \\mu_{manual} - \\mu_{automatic} \\ne 0$, we can reject the null hypothesis given the p-value, which means that the data provides strong evidence that there is an actual difference in fuel efficiency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.29 - Chicken diet and weight, Part II.\n",
    "\n",
    "Let's calculate $SE = 23.56$ and $df = 11$. Now we get $T = 3.27$ and $p\\_value = 0.007$ which makes us reject the null hypothesis according to which there is no difference in average weight between the two groups. So it is safe to say that the casein diet is responsible for the higher average weight."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.30 - Fuel efficiency of manual and automatic cars, Part II.\n",
    "\n",
    "The _98% Confidence Interval_ for the difference $\\mu_{manual} - \\mu_{automatic}$ is $[1.4090783673065097,\\ 8.510921632693485]$ so we can say that we are 98% confident that the average highway fuel efficienty for manual cars is higher than the one for automatic cars by an amount within the found confidence interval."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.31 - Prison isolation experiment, Part I.\n",
    "\n",
    "We need to check if normality is met first: for _Tr 2_ we have a pretty normal distribution while the other two treatments does not really show a normal trend and therefore normality is a strong assumption to make. The null hypothesis is that there is no difference among the average scores from each treatments. We can compare them two by two, remembering that the null hypothesis is that the average scores are the same, while the alternate hypothesis is that there's an actual difference between the two averages. We want to use $\\alpha = 0.05$.\n",
    "\n",
    "* Let's first compare $\\mu_{Tr_1}$ with $\\mu_{Tr_2}$. Let's calculate, as per usual, $SE = 3.91$, $df = 14 - 1 = 13$, $T = 0.86$ and $p\\_value = 0.41$. So there's not sufficient evidence to reject the null hypothesis from this dataset.\n",
    "* Let's first compare $\\mu_{Tr_1}$ with $\\mu_{Tr_3}$. Let's calculate, as per usual, $SE = 4.01$, $df = 14 - 1 = 13$, $T = 2.35$ and $p\\_value = 0.04$. So there's strong evidence suggesting an actual difference in average score between _Tr 1_ and _Tr 2_.\n",
    "* Let's first compare $\\mu_{Tr_2}$ with $\\mu_{Tr_3}$. Let's calculate, as per usual, $SE = 3.12$, $df = 14 - 1 = 13$, $T = 1.94$ and $p\\_value = 0.07$. So there is not sufficient evidence to assess that treatment _Tr 2_ and _Tr 3_ average scores differ significantly, i.e. there is no evidence to reject the null hypothesis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.32 - True / False: comparing means.\n",
    "\n",
    "* (a) False, we need to check the appropriate conditions (i.e. for $n < 30$ that no clear outliers are present while for $n \\ge 30$ that no extreme outliers are present).\n",
    "* (b) True, as the distribution approaches a normal distribution (given the number of samples increasing).\n",
    "* (c) No, we use a pooled standard error when the standard deviations are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE = 3.1223674625880555, df = 13, cv (alpha = 0.02) = 2.6503088378527013, t statistic = 1.9440376806158244, p-value = 0.07385522761200089 Confidence interval = (-2.2052380811208376, 14.345238081120838)\n"
     ]
    }
   ],
   "source": [
    "# Cell to work out the math\n",
    "\n",
    "m1, m2 = 2.86, -3.21\n",
    "s1, s2 = 7.94, 8.57\n",
    "n1, n2 = 14, 14\n",
    "\n",
    "diff_mean = m1 - m2\n",
    "\n",
    "alpha = 0.02\n",
    "\n",
    "se = se_diff(s1, s2, n1, n2)\n",
    "df = df_diff(n1, n2)\n",
    "\n",
    "cv = t.ppf(1 - alpha / 2, df)\n",
    "\n",
    "t_stat = (m1 - m2) / se \n",
    "p_val = t.sf(abs(t_stat), df) * 2\n",
    "\n",
    "print(f\"SE = {se}, df = {df}, cv (alpha = {alpha}) = {cv}, t statistic = {t_stat}, p-value = {p_val} Confidence interval = {diff_mean - cv * se, diff_mean + cv * se}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fa8e7a0e7c7188de72acea4ae1bc222d1770499c4c3d36ce32843ef46b20053"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
